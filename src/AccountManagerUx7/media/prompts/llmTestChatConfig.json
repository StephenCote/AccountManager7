{
    "schema": "olio.llm.chatConfig",
    "name": "LLM Test Config",
    "model": "way-local",
    "serverUrl": "http://192.168.1.42:11434",
    "serviceType": "OLLAMA",
    "messageTrim": 6,
    "remindEvery": 4,
    "keyframeEvery": 3,
    "stream": true,
    "prune": true,
    "rating": "E",
    "assist": true,
    "startMode": "none",
    "chatOptions": {
        "schema": "olio.llm.chatOptions",
        "temperature": 0.8,
        "top_p": 0.9,
        "top_k": 50,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "max_tokens": 4096,
        "num_ctx": 8192,
        "seed": 0
    }
}
