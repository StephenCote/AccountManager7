{
	"name": "olio.llm.openai.openaiRequest",
	"likeInherits": [
		"data.directory"
	],
	"inherits": [
		"common.groupExt",
		"common.baseLight",
		"olio.llm.request"
	],
	"description": "An OpenAI request model that defines the parameters for making requests to OpenAI's language models. This includes settings like temperature, max tokens, and the messages to be processed by the model.",
	"group": "Chat",
	"icon": "chat",
	"label": "Requests",
	"vector": "org.cote.accountmanager.olio.VectorProvider",
	"query": [
		"id",
		"groupId"
	],
	"fields": [
		{
			"name": "max_completion_tokens",
			"type": "int",
			"default": 2048,
			"minValue": 0,
			"maxValue": 120000,
			"description": "For o1 and later models"
		},
		{
			"name": "max_tokens",
			"type": "int",
			"default": 2048,
			"minValue": 0,
			"maxValue": 120000
		},
		{
			"name": "num_ctx",
			"type": "int",
			"default": 2048,
			"minValue": 0,
			"maxValue": 120000,
			"description": "Cross compat for Ollama OpenAI"
		},
	    {
			"name": "temperature",
			"type": "double",
			"default": 0.75
		},
		{
			"name": "top_p",
			"type": "double",
			"default": 0.5
		},
		{
			"name": "frequency_penalty",
			"type": "double",
			"default": 0.0
		},
		{
			"name": "presence_penalty",
			"type": "double",
			"default": 0.0
		},
		{
			"name": "seed",
			"type": "int",
			"default": 0
		},
		{
			"name": "messages",
			"type": "list",
			"baseType": "model",
			"baseModel": "olio.llm.openai.openaiMessage"
		}
	]
}
